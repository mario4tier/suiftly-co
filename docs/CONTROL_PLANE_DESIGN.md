# Control Plane Design

## Overview

The Suiftly control plane manages configuration distribution and status collection across all servers. It spans two repositories:

| Component | Repository | Runs On | API Port |
|-----------|------------|---------|----------|
| **Global Manager** | `~/suiftly-co` | Primary server (with PostgreSQL) | 22600 |
| **Local Manager** | `~/walrus` | All servers (including primary) | 22610-22613 |
| **VAULT files** | `~/walrus` | Generated by Global Manager, consumed by Local Manager | N/A |

**Port Allocation:** See [~/walrus/PORT_MAP.md](~/walrus/PORT_MAP.md) for the single source of truth on port assignments. Local Managers use ports 22610-22613 to support up to 4 instances in development for multi-server simulation.

**Primary Server (EU-West NetOps):**
```
~/suiftly-co/  → Global Manager, API, Webapp, PostgreSQL
~/walrus/      → Local Manager, HAProxy, keyserver, VAULT simulation
```

**Other Gateway Servers (multiple regions):**
```
~/walrus/      → Local Manager, HAProxy, keyserver (no suiftly-co)
```

---

## Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Primary Server (EU-West NetOps)                      │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                         ~/suiftly-co                                  │  │
│  │  ┌─────────────────────────────────────────────────────────────────┐  │  │
│  │  │              Global Manager (daemon, 5-min cycle)               │  │  │
│  │  │                                                                 │  │  │
│  │  │  1. Aggregate HAProxy logs → usage metrics                      │  │  │
│  │  │  2. Calculate customer billing                                  │  │  │
│  │  │  3. Generate MA_VAULT (API keys + rate limits)                  │  │  │
│  │  │  4. Aggregate status from all Local Managers                    │  │  │
│  │  │  5. Cleanup old data                                            │  │  │
│  │  └─────────────────────────────────────────────────────────────────┘  │  │
│  │                              ↓                                        │  │
│  │                    PostgreSQL (source of truth)                       │  │
│  │                              ↓                                        │  │
│  │                    API Server → Webapp (UI)                           │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                 ↓                                           │
│                    MA_VAULT files (to /opt/syncf/data_tx/ma/)               │
│                                 ↓                                           │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                           ~/walrus                                    │  │
│  │  sync-files.py → distributes VAULT to all servers                     │  │
│  │  Local Manager → consumes VAULT, updates HAProxy                      │  │
│  │  HAProxy → enforces rate limits, logs requests                        │  │
│  │  Fluentd → ships logs back to PostgreSQL                              │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
                                  ↓
                        (rsync via sync-files.py)
                                  ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                      Other Gateway Servers (regions)                        │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                           ~/walrus                                    │  │
│  │  Local Manager → consumes VAULT, updates HAProxy                      │  │
│  │  HAProxy → enforces rate limits, logs requests                        │  │
│  │  Fluentd → ships logs back to PostgreSQL                              │  │
│  │  Status Reporter → reports health to Global Manager                   │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Data Flows

### 1. Configuration Distribution (Global → Local)

```
Database (customers, API keys, tiers)
           ↓
Global Manager (generates MA_VAULT)
           ↓
/opt/syncf/data_tx/ma/*.enc files
           ↓
sync-files.py (rsync to all servers)
           ↓
Local Manager (reads VAULT, updates HAProxy)
           ↓
HAProxy (enforces rate limits per customer)
```

### 2. Usage Collection (Local → Global)

```
Customer Request
       ↓
HAProxy (logs request with API key, bytes, latency)
       ↓
Syslog (UDP)
       ↓
Fluentd (1s batches)
       ↓
PostgreSQL (haproxy_logs table, TimescaleDB)
       ↓
Global Manager (aggregates into usage_hourly)
       ↓
Billing calculation
```

### 3. Status Reporting (Local → Global → UI)

```
Local Manager (checks HAProxy, keyserver health)
       ↓
Status API call to Global Manager
       ↓
PostgreSQL (server_status table)
       ↓
Global Manager (aggregates fleet health)
       ↓
API Server
       ↓
Webapp (admin dashboard)
```

---

## Global Manager (This Repository)

Long-lived daemon process running on the primary database server. Handles all centralized operations.

### Responsibilities

1. **Metering** - Aggregates HAProxy logs into usage metrics
2. **Billing** - Calculates customer charges from usage data
3. **Vault Generation** - Generates MA_VAULT (customer API keys and rate limits)
4. **Status Aggregation** - Collects and aggregates status from all Local Managers
5. **Data Cleanup** - Removes old logs and maintains database health

### Key Characteristics

- **Long-lived daemon** - Runs continuously as a systemd service
- **Singleton** - Only one instance runs at a time (enforced by PostgreSQL advisory locks)
- **Idempotent** - Safe to run multiple times without side effects
- **Periodic** - Internal setInterval loop runs tasks every 5 minutes
- **Crash-safe** - Uses database transactions for atomicity
- **Resumable** - Picks up where it left off after failures
- **Graceful shutdown** - Handles SIGTERM/SIGINT properly

### Implementation

```typescript
// services/global-manager/src/index.ts

import { db } from '@suiftly/database'
import { acquireLock, releaseLock } from './lib/lock'
import { aggregateLogs } from './tasks/aggregate-logs'
import { calculateBills } from './tasks/calculate-bills'
import { generateMAVault } from './tasks/generate-ma-vault'
import { aggregateStatus } from './tasks/aggregate-status'
import { cleanup } from './tasks/cleanup'
import pino from 'pino'

const logger = pino({ name: 'global-manager' })

const CONFIG = {
  LOCK_ID: 1001,  // PostgreSQL advisory lock
  RUN_INTERVAL_MS: 5 * 60 * 1000,  // 5 minutes
  DRY_RUN: process.env.DRY_RUN === 'true'
}

let isShuttingDown = false

// Main periodic task runner (runs each cycle)
async function runGlobalManager() {
  const startTime = Date.now()
  logger.info('Starting Global Manager cycle')

  let lockAcquired = false

  try {
    // Acquire exclusive lock (skip if another instance is running)
    lockAcquired = await acquireLock(CONFIG.LOCK_ID)
    if (!lockAcquired) {
      logger.warn('Another instance is running, skipping this cycle')
      return
    }

    // Run tasks in sequence (each is idempotent)
    await runWithMetrics('aggregate-logs', aggregateLogs)
    await runWithMetrics('calculate-bills', calculateBills)
    await runWithMetrics('generate-ma-vault', generateMAVault)
    await runWithMetrics('aggregate-status', aggregateStatus)
    await runWithMetrics('cleanup', cleanup)

    const duration = Date.now() - startTime
    logger.info({ duration }, 'Global Manager cycle completed')

  } catch (error) {
    logger.error({ error }, 'Global Manager cycle failed')
    // Don't throw - we want to continue running and try again next cycle
  } finally {
    if (lockAcquired) {
      await releaseLock(CONFIG.LOCK_ID)
    }
  }
}

// Scheduler loop (runs forever until shutdown)
async function scheduler() {
  logger.info({ intervalMs: CONFIG.RUN_INTERVAL_MS }, 'Scheduler started')

  while (!isShuttingDown) {
    try {
      await runGlobalManager()
    } catch (error) {
      logger.error({ error }, 'Unexpected error in scheduler')
    }

    if (!isShuttingDown) {
      logger.info(`Waiting ${CONFIG.RUN_INTERVAL_MS / 1000}s until next cycle`)
      await sleep(CONFIG.RUN_INTERVAL_MS)
    }
  }

  logger.info('Scheduler stopped')
}

// Helper: sleep with early exit on shutdown
function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => {
    const timeout = setTimeout(resolve, ms)
    const checkShutdown = setInterval(() => {
      if (isShuttingDown) {
        clearTimeout(timeout)
        clearInterval(checkShutdown)
        resolve()
      }
    }, 100)
  })
}

// Helper: track task metrics
async function runWithMetrics(taskName: string, task: () => Promise<void>) {
  const startTime = Date.now()
  logger.info({ task: taskName }, 'Starting task')

  try {
    await task()
    const duration = Date.now() - startTime

    await db.insert(worker_runs).values({
      worker_type: 'global-manager',
      task_name: taskName,
      status: 'success',
      duration_ms: duration,
      executed_at: new Date()
    })

    logger.info({ task: taskName, duration }, 'Task completed')
  } catch (error) {
    logger.error({ task: taskName, error }, 'Task failed')

    await db.insert(worker_runs).values({
      worker_type: 'global-manager',
      task_name: taskName,
      status: 'failed',
      error_message: error.message,
      executed_at: new Date()
    })

    throw error
  }
}

// Graceful shutdown handling
function setupShutdownHandlers() {
  const shutdown = async (signal: string) => {
    logger.info({ signal }, 'Shutdown signal received')
    isShuttingDown = true

    const maxWait = 30000
    const startWait = Date.now()
    while (Date.now() - startWait < maxWait) {
      await sleep(1000)
    }

    logger.info('Global Manager shutdown complete')
    process.exit(0)
  }

  process.on('SIGTERM', () => shutdown('SIGTERM'))
  process.on('SIGINT', () => shutdown('SIGINT'))
}

// Start the daemon
async function main() {
  logger.info('Global Manager daemon starting...')
  setupShutdownHandlers()
  await scheduler()
}

main().catch((error) => {
  logger.fatal({ error }, 'Fatal error in Global Manager')
  process.exit(1)
})
```

### Task: Aggregate Logs

```typescript
// services/global-manager/src/tasks/aggregate-logs.ts

export async function aggregateLogs() {
  // Get last processed timestamp
  const lastProcessed = await db.query.processing_state.findFirst({
    where: eq(processing_state.key, 'last_aggregated_log_timestamp')
  })

  const fromTimestamp = lastProcessed?.value || '2025-01-01T00:00:00Z'
  const toTimestamp = new Date().toISOString()

  // Aggregate into hourly buckets (idempotent via UPSERT)
  await db.execute(sql`
    INSERT INTO usage_hourly (hour, customer_id, service_type, request_count, bytes_in, bytes_out)
    SELECT
      date_trunc('hour', timestamp) as hour,
      customer_id,
      service_type,
      COUNT(*) as request_count,
      SUM(bytes_in) as bytes_in,
      SUM(bytes_out) as bytes_out
    FROM haproxy_logs
    WHERE timestamp > ${fromTimestamp}
      AND timestamp <= ${toTimestamp}
    GROUP BY hour, customer_id, service_type
    ON CONFLICT (hour, customer_id, service_type)
    DO UPDATE SET
      request_count = usage_hourly.request_count + EXCLUDED.request_count,
      bytes_in = usage_hourly.bytes_in + EXCLUDED.bytes_in,
      bytes_out = usage_hourly.bytes_out + EXCLUDED.bytes_out
  `)

  // Update last processed timestamp
  await db.insert(processing_state)
    .values({
      key: 'last_aggregated_log_timestamp',
      value: toTimestamp
    })
    .onConflictDoUpdate({
      target: processing_state.key,
      set: { value: toTimestamp, updated_at: new Date() }
    })
}
```

### Task: Generate MA_VAULT

```typescript
// services/global-manager/src/tasks/generate-ma-vault.ts
import { execSync } from 'child_process'

export async function generateMAVault() {
  // Get all active customers with their services and API keys
  const customers = await db.query.customers.findMany({
    where: eq(customers.status, 'active'),
    with: {
      api_keys: true,
      service_instances: true
    }
  })

  // Build MA_VAULT data structure (key-value format for kvcrypt)
  const vaultData: Record<string, string> = {}

  for (const customer of customers) {
    const sealService = customer.service_instances.find(s => s.service_type === 'seal')
    const tier = sealService?.tier || 'starter'

    const customerConfig = {
      api_keys: customer.api_keys.map(k => k.api_key_fp),
      tier,
      limits: getTierLimits(tier),
      status: customer.status
    }

    vaultData[`customer:${customer.customer_id}`] = JSON.stringify(customerConfig)
  }

  // Generate content hash for change detection
  const vaultContent = JSON.stringify(vaultData, null, 2)
  const vaultHash = crypto.createHash('sha256').update(vaultContent).digest('hex')

  // Check if this version already exists
  const existing = await db.query.vault_versions.findFirst({
    where: eq(vault_versions.content_hash, vaultHash)
  })

  if (!existing) {
    // New version - write to MA_VAULT using kvcrypt (from ~/walrus)
    for (const [key, value] of Object.entries(vaultData)) {
      execSync(`~/walrus/scripts/sync/kvcrypt.py put ma "${key}" '${value}'`, {
        stdio: 'pipe'
      })
    }

    await db.insert(vault_versions).values({
      version: Date.now(),
      content_hash: vaultHash,
      customer_count: customers.length,
      created_at: new Date()
    })

    logger.info({ hash: vaultHash, count: customers.length }, 'New MA_VAULT version generated')
    // sync-files.py (systemd timer in ~/walrus) distributes to all servers
  } else {
    logger.info({ hash: vaultHash }, 'MA_VAULT unchanged, skipping generation')
  }
}
```

### Task: Aggregate Status

```typescript
// services/global-manager/src/tasks/aggregate-status.ts

export async function aggregateStatus() {
  // Get latest status from all servers (reported by Local Managers)
  const serverStatuses = await db.query.server_status.findMany({
    where: gte(server_status.reported_at, sql`NOW() - INTERVAL '10 minutes'`)
  })

  // Calculate fleet health
  const totalServers = serverStatuses.length
  const healthyServers = serverStatuses.filter(s => s.status === 'healthy').length
  const degradedServers = serverStatuses.filter(s => s.status === 'degraded').length
  const unhealthyServers = serverStatuses.filter(s => s.status === 'unhealthy').length

  // Update fleet status
  await db.insert(fleet_status)
    .values({
      total_servers: totalServers,
      healthy_servers: healthyServers,
      degraded_servers: degradedServers,
      unhealthy_servers: unhealthyServers,
      updated_at: new Date()
    })
    .onConflictDoUpdate({
      target: fleet_status.id,
      set: {
        total_servers: totalServers,
        healthy_servers: healthyServers,
        degraded_servers: degradedServers,
        unhealthy_servers: unhealthyServers,
        updated_at: new Date()
      }
    })

  // Log any unhealthy servers
  const unhealthy = serverStatuses.filter(s => s.status === 'unhealthy')
  if (unhealthy.length > 0) {
    logger.warn({ servers: unhealthy.map(s => s.server_name) }, 'Unhealthy servers detected')
  }
}
```

---

## Local Manager (~/walrus Repository)

The Local Manager runs on every server and is documented in the walrus project. Key responsibilities:

1. **Consume VAULT** - Read MA_VAULT files distributed by sync-files.py
2. **Update HAProxy** - Apply rate limits and API key configurations
3. **Report Status** - Send health information to Global Manager
4. **Manage keyservers** - Coordinate Seal encryption services

**See ~/walrus project documentation for:**
- VAULT format specification (kvcrypt encrypted key-value store)
- Local Manager implementation
- HAProxy configuration templates
- Fluentd log shipping setup
- sync-files.py distribution mechanism

---

## Integration Points

### VAULT Interface

Global Manager writes VAULT using `kvcrypt.py` from ~/walrus:

```bash
# Write customer config to MA_VAULT
~/walrus/scripts/sync/kvcrypt.py put ma "customer:12345" '{"api_keys":["fp1","fp2"],"tier":"pro"}'

# Files created in /opt/syncf/data_tx/ma/
# sync-files.py distributes to all servers
```

**VAULT format is defined in ~/walrus** - this repository only produces data in that format.

### Status Reporting Interface

Local Managers report status via HTTP POST to the API:

```typescript
// Expected payload from Local Manager
interface ServerStatusReport {
  server_name: string
  region: string
  status: 'healthy' | 'degraded' | 'unhealthy'
  haproxy_status: 'running' | 'stopped' | 'error'
  keyserver_status: 'running' | 'stopped' | 'error'
  vault_version: number
  last_vault_sync: string  // ISO timestamp
  metrics: {
    requests_per_second: number
    active_connections: number
    error_rate: number
  }
}
```

### Log Shipping Interface

Fluentd ships logs from HAProxy to PostgreSQL:

```sql
-- Expected log format (TimescaleDB hypertable)
INSERT INTO haproxy_logs (
  timestamp, customer_id, api_key_id, service_type,
  endpoint, method, status_code, bytes_in, bytes_out,
  request_time_ms, backend_time_ms, haproxy_server, region
) VALUES (...)
```

---

## Database Schema

### Tables Managed by Global Manager

```sql
-- Processing state tracking
CREATE TABLE processing_state (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Worker run history
CREATE TABLE worker_runs (
  id SERIAL PRIMARY KEY,
  worker_type TEXT NOT NULL,  -- 'global-manager'
  task_name TEXT NOT NULL,
  status TEXT NOT NULL,
  duration_ms INT,
  error_message TEXT,
  executed_at TIMESTAMPTZ DEFAULT NOW()
);

-- MA_VAULT versions
CREATE TABLE vault_versions (
  id SERIAL PRIMARY KEY,
  version BIGINT NOT NULL,
  content_hash TEXT NOT NULL UNIQUE,
  customer_count INT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Server status (reported by Local Managers)
CREATE TABLE server_status (
  server_name TEXT PRIMARY KEY,
  region TEXT NOT NULL,
  status TEXT NOT NULL,
  haproxy_status TEXT,
  keyserver_status TEXT,
  vault_version BIGINT,
  last_vault_sync TIMESTAMPTZ,
  metrics JSONB DEFAULT '{}',
  reported_at TIMESTAMPTZ DEFAULT NOW()
);

-- Fleet status (aggregated by Global Manager)
CREATE TABLE fleet_status (
  id INTEGER PRIMARY KEY DEFAULT 1,
  total_servers INT,
  healthy_servers INT,
  degraded_servers INT,
  unhealthy_servers INT,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Tables for Usage/Billing

See [CUSTOMER_SERVICE_SCHEMA.md](CUSTOMER_SERVICE_SCHEMA.md) for complete schema:
- `haproxy_logs` - TimescaleDB hypertable for raw logs
- `usage_hourly` - Aggregated usage metrics
- `billing_records` - Customer billing records

---

## Systemd Configuration

### Global Manager Service

```ini
# /etc/systemd/system/suiftly-global-manager.service
[Unit]
Description=Suiftly Global Manager (Long-Lived Daemon)
After=network.target postgresql.service
Requires=postgresql.service

[Service]
Type=simple
User=deploy
Group=deploy
WorkingDirectory=/var/www/global-manager

Environment="NODE_ENV=production"
Environment="DATABASE_URL=postgresql://deploy@localhost/suiftly_prod"

ExecStart=/usr/bin/node dist/index.js

Restart=always
RestartSec=10s

KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=35s

MemoryMax=512M
CPUQuota=50%

StandardOutput=journal
StandardError=journal
SyslogIdentifier=suiftly-global-manager

PrivateTmp=yes
NoNewPrivileges=yes
ProtectSystem=strict
ProtectHome=yes
ReadWritePaths=/opt/syncf/data_tx

[Install]
WantedBy=multi-user.target
```

---

## Development & Testing

### Running Both Repositories Locally

```bash
# Terminal 1: Start suiftly-co (API, Webapp, DB)
cd ~/suiftly-co
./scripts/dev/start-dev.sh

# Terminal 2: Start walrus (Local Manager simulation)
cd ~/walrus
./scripts/dev/start-local-manager.sh --mock

# Terminal 3: Run Global Manager manually
cd ~/suiftly-co
npm run global-manager:dev
```

### E2E Test Flow

The walrus project provides VAULT simulation and multi-server mocking. To test the full control plane:

1. **Start suiftly-co services** (API, Webapp)
2. **Start walrus simulation** (mock HAProxy, keyservers, Local Manager)
3. **Trigger Global Manager cycle** (generates VAULT)
4. **Verify Local Manager receives VAULT** (walrus logs)
5. **Verify status flows back** (check server_status table)
6. **Verify UI displays status** (webapp admin dashboard)

```bash
# Run E2E control plane tests
cd ~/suiftly-co
npm run test:control-plane

# This script:
# 1. Starts both repos in test mode
# 2. Inserts test customer data
# 3. Runs Global Manager cycle
# 4. Verifies VAULT generation
# 5. Verifies status aggregation
# 6. Cleans up
```

### Mock Architecture

```
~/suiftly-co (this repo)           ~/walrus (mock mode)
┌─────────────────────┐            ┌─────────────────────┐
│ Global Manager      │───VAULT───→│ Local Manager       │
│ (real)              │            │ (real)              │
│                     │            │                     │
│ PostgreSQL          │←──status───│ Mock HAProxy        │
│ (real)              │            │ (simulated)         │
│                     │            │                     │
│ API Server          │←──logs─────│ Mock Fluentd        │
│ (real)              │            │ (simulated)         │
└─────────────────────┘            └─────────────────────┘
```

---

## Monitoring & Health Checks

### Admin Dashboard

Simple HTML dashboard on port 22600 (localhost only):

```typescript
// services/global-manager/src/admin-server.ts
fastify.get('/', async (req, reply) => {
  const lastRuns = await db.query.worker_runs.findMany({
    where: eq(worker_runs.worker_type, 'global-manager'),
    orderBy: desc(worker_runs.executed_at),
    limit: 20
  })

  const fleetStatus = await db.query.fleet_status.findFirst()

  return reply.type('text/html').send(`
    <h1>Control Plane Status</h1>
    <h2>Fleet Health</h2>
    <p>Healthy: ${fleetStatus.healthy_servers}/${fleetStatus.total_servers}</p>
    <h2>Recent Global Manager Runs</h2>
    <table>...</table>
  `)
})
```

### Health Check Endpoint

```typescript
// apps/api/src/routes/health.ts
router.get('/health/control-plane', async (req, res) => {
  const [lastRun, fleetStatus] = await Promise.all([
    db.query.worker_runs.findFirst({
      where: and(
        eq(worker_runs.worker_type, 'global-manager'),
        eq(worker_runs.status, 'success')
      ),
      orderBy: desc(worker_runs.executed_at)
    }),
    db.query.fleet_status.findFirst()
  ])

  const minutesSinceRun = (Date.now() - lastRun.executed_at.getTime()) / 60000
  const unhealthyRatio = fleetStatus.unhealthy_servers / fleetStatus.total_servers

  if (minutesSinceRun > 15 || unhealthyRatio > 0.5) {
    return res.status(503).send({ status: 'unhealthy' })
  }

  return res.send({
    status: 'healthy',
    global_manager_last_run: lastRun.executed_at,
    fleet: fleetStatus
  })
})
```

### Monitoring Commands

```bash
# Global Manager status
systemctl status suiftly-global-manager.service
journalctl -u suiftly-global-manager -f

# Recent runs
psql suiftly_prod -c "SELECT * FROM worker_runs ORDER BY executed_at DESC LIMIT 10;"

# Fleet status
psql suiftly_prod -c "SELECT * FROM fleet_status;"

# Server status (from Local Managers)
psql suiftly_prod -c "SELECT * FROM server_status ORDER BY reported_at DESC;"
```

---

## Security Considerations

1. **VAULT Encryption** - AES-256-GCM via kvcrypt (~/walrus)
2. **PostgreSQL Advisory Locks** - Prevent concurrent Global Manager execution
3. **Systemd Security** - ProtectSystem, PrivateTmp, limited permissions
4. **API Key Hashing** - Only fingerprints stored in VAULT, not raw keys
5. **Internal Network** - Status reporting over private network only
6. **Audit Trail** - All VAULT versions and configuration changes logged

---

## Disaster Recovery

### Global Manager Failure

- Advisory lock expires automatically
- Next cycle continues from last processed state
- Local Managers continue with cached VAULT

### Local Manager Failure

- HAProxy continues with last known config
- Status goes stale (detected by Global Manager)
- Alert generated for unhealthy server

### Database Failure

- Global Manager retries with exponential backoff
- Local Managers use cached VAULT
- Logs buffer in Fluentd until DB recovers

### Network Partition

- Local Managers operate independently
- VAULT updates queued until connectivity restored
- Status reporting resumes automatically

---

## Future Enhancements

1. **Real-time Updates** - WebSocket push for instant rate limit changes
2. **Multi-region Aggregation** - Regional billing before global aggregation
3. **Anomaly Detection** - ML-based usage pattern analysis
4. **Live Subscriptions** - GraphQL subscriptions for dashboard updates
