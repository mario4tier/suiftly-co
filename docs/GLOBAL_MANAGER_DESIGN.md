# Global Manager Design

## Overview

The **Global Manager** is a centralized worker process that runs on the primary database server (co-located with PostgreSQL). It performs critical periodic operations for the entire Suiftly platform:

**Core Responsibilities:**
1. **Metering** - Aggregates HAProxy logs into usage metrics
2. **Billing** - Calculates customer charges from usage data
3. **Vault Generation** - Generates MA_VAULT (customer API keys and rate limits)
4. **Data Cleanup** - Removes old logs and maintains database health
5. **Key Management** - Generates MM_VAULT (imported encryption keys - optional, see appendix)

**Key Characteristics:**
- **Long-lived daemon** - Runs continuously as a systemd service
- **Singleton** - Only one instance runs at a time (enforced by PostgreSQL advisory locks)
- **Idempotent** - Safe to run multiple times without side effects
- **Periodic** - Internal setInterval loop runs tasks every 5 minutes
- **Crash-safe** - Uses database transactions for atomicity
- **Resumable** - Picks up where it left off after failures
- **Graceful shutdown** - Handles SIGTERM/SIGINT properly

**Terminology:**
- **MA_VAULT** - Master Access vault containing critical customer configurations (API keys, rate limits, status)
- **MM_VAULT** - Master Mainnet vault containing encryption keys for Seal service (less critical, see appendix)
- **Local Manager** - Distributed workers in the walrus project that consume MA_VAULT to update HAProxy configurations

**Implementation:** TypeScript with Node.js 22 LTS, deployed as a systemd daemon service for maximum reliability.

**Note:** Local Managers are part of the walrus project and are not documented here. They consume the MA_VAULT generated by the Global Manager.

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     PostgreSQL Server                       │
│  ┌─────────────────────────────────────────────────────┐    │
│  │     Global Manager (daemon, setInterval loop)       │    │
│  │                                                     │    │
│  │  Core Operations (every 5 minutes):                │    │
│  │  1. Aggregate HAProxy logs → usage metrics         │    │
│  │  2. Calculate customer billing                     │    │
│  │  3. Generate MA_VAULT (API keys + rate limits)     │    │
│  │  4. Generate MM_VAULT (encryption keys, optional)  │    │
│  │  5. Cleanup old data                               │    │
│  │                                                     │    │
│  │  Guarantees:                                       │    │
│  │  • Single instance (PostgreSQL advisory lock)     │    │
│  │  • Idempotent operations                          │    │
│  │  • Crash-safe (database transactions)             │    │
│  │  • Resumable (tracks processing state)            │    │
│  └─────────────────────────────────────────────────────┘    │
│                           ↓                                 │
│              MA_VAULT files (encrypted, versioned)          │
└─────────────────────────────────────────────────────────────┘
                           ↓
                    (sync-files.py distributes
                     to API Gateway servers
                     in ~/walrus project)
                           ↓
┌────────────────────────────────────────────────────────────┐
│              API Gateway Servers (~/walrus)                │
│  - Local Managers consume MA_VAULT                         │
│  - HAProxy enforces rate limits                            │
│  - Fluentd ships logs back to PostgreSQL                   │
│  (See ~/walrus project documentation for details)          │
└────────────────────────────────────────────────────────────┘
```

## Data Flow

```
                    ┌─────────────────────┐
                    │  Customer Request   │
                    └──────────┬──────────┘
                               ↓
              ┌────────────────────────────────┐
              │  HAProxy (~/walrus project)    │
              │  - Logs request                │
              │  - Enforces rate limits        │
              └────────┬───────────────────────┘
                       ↓
                   Syslog (UDP)
                       ↓
              ┌────────────────┐
              │    Fluentd     │
              │  (1s batches)  │
              └────────┬───────┘
                       ↓
         ┌─────────────────────────────┐
         │  PostgreSQL + TimescaleDB   │
         │  - haproxy_logs table       │
         └─────────┬───────────────────┘
                   ↓
    ┌──────────────────────────────────────┐
    │      Global Manager (5 min)          │
    │  1. Aggregate logs → usage_hourly    │
    │  2. Calculate billing                │
    │  3. Generate MA_VAULT                │
    │  4. Generate MM_VAULT (optional)     │
    └──────────┬───────────────────────────┘
               ↓
       MA_VAULT files (encrypted)
               ↓
    (sync-files.py distributes to
     API Gateway servers in ~/walrus)
               ↓
       Local Managers (~/walrus)
       update HAProxy configurations
```

---

## 1. Global Manager (Centralized)

Long-lived daemon process running on the primary database server. Handles all centralized operations with internal periodic scheduling using setInterval.

### Implementation

```typescript
// services/global-manager/src/index.ts

import { db } from '@suiftly/database'
import { acquireLock, releaseLock } from './lib/lock'
import { aggregateLogs } from './tasks/aggregate-logs'
import { calculateBills } from './tasks/calculate-bills'
import { generateMAVault } from './tasks/generate-ma-vault'
import { generateMMVault } from './tasks/generate-mm-vault'
import { cleanup } from './tasks/cleanup'
import pino from 'pino'

const logger = pino({ name: 'global-manager' })

const CONFIG = {
  LOCK_ID: 1001,  // PostgreSQL advisory lock
  RUN_INTERVAL_MS: 5 * 60 * 1000,  // 5 minutes
  DRY_RUN: process.env.DRY_RUN === 'true'
}

let isShuttingDown = false

// Main periodic task runner (runs each cycle)
async function runGlobalManager() {
  const startTime = Date.now()
  logger.info('Starting Global Manager cycle')

  let lockAcquired = false

  try {
    // Acquire exclusive lock (skip if another instance is running)
    lockAcquired = await acquireLock(CONFIG.LOCK_ID)
    if (!lockAcquired) {
      logger.warn('Another instance is running, skipping this cycle')
      return
    }

    // Run tasks in sequence (each is idempotent)
    await runWithMetrics('aggregate-logs', aggregateLogs)
    await runWithMetrics('calculate-bills', calculateBills)
    await runWithMetrics('generate-ma-vault', generateMAVault)
    await runWithMetrics('generate-mm-vault', generateMMVault)  // Optional
    await runWithMetrics('cleanup', cleanup)

    const duration = Date.now() - startTime
    logger.info({ duration }, 'Global Manager cycle completed')

  } catch (error) {
    logger.error({ error }, 'Global Manager cycle failed')
    // Don't throw - we want to continue running and try again next cycle
  } finally {
    if (lockAcquired) {
      await releaseLock(CONFIG.LOCK_ID)
    }
  }
}

// Scheduler loop (runs forever until shutdown)
async function scheduler() {
  logger.info({ intervalMs: CONFIG.RUN_INTERVAL_MS }, 'Scheduler started')

  while (!isShuttingDown) {
    try {
      await runGlobalManager()
    } catch (error) {
      // Should be caught inside runGlobalManager, but double-check
      logger.error({ error }, 'Unexpected error in scheduler')
    }

    // Wait for next interval (unless shutting down)
    if (!isShuttingDown) {
      logger.info(`Waiting ${CONFIG.RUN_INTERVAL_MS / 1000}s until next cycle`)
      await sleep(CONFIG.RUN_INTERVAL_MS)
    }
  }

  logger.info('Scheduler stopped')
}

// Helper: sleep with early exit on shutdown
function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => {
    const timeout = setTimeout(resolve, ms)

    // Check for shutdown signal periodically
    const checkShutdown = setInterval(() => {
      if (isShuttingDown) {
        clearTimeout(timeout)
        clearInterval(checkShutdown)
        resolve()
      }
    }, 100)
  })
}

// Helper: track task metrics
async function runWithMetrics(taskName: string, task: () => Promise<void>) {
  const startTime = Date.now()
  logger.info({ task: taskName }, 'Starting task')

  try {
    await task()
    const duration = Date.now() - startTime

    await db.insert(worker_runs).values({
      worker_type: 'global-manager',
      task_name: taskName,
      status: 'success',
      duration_ms: duration,
      executed_at: new Date()
    })

    logger.info({ task: taskName, duration }, 'Task completed')
  } catch (error) {
    logger.error({ task: taskName, error }, 'Task failed')

    await db.insert(worker_runs).values({
      worker_type: 'global-manager',
      task_name: taskName,
      status: 'failed',
      error_message: error.message,
      executed_at: new Date()
    })

    throw error
  }
}

// Graceful shutdown handling
function setupShutdownHandlers() {
  const shutdown = async (signal: string) => {
    logger.info({ signal }, 'Shutdown signal received')
    isShuttingDown = true

    // Wait for current cycle to finish (max 30s)
    const maxWait = 30000
    const startWait = Date.now()

    while (Date.now() - startWait < maxWait) {
      await sleep(1000)
    }

    logger.info('Global Manager shutdown complete')
    process.exit(0)
  }

  process.on('SIGTERM', () => shutdown('SIGTERM'))
  process.on('SIGINT', () => shutdown('SIGINT'))
}

// Start the daemon
async function main() {
  logger.info('Global Manager daemon starting...')

  setupShutdownHandlers()

  // Start scheduler loop (runs forever)
  await scheduler()
}

main().catch((error) => {
  logger.fatal({ error }, 'Fatal error in Global Manager')
  process.exit(1)
})
```

### Task: Aggregate Logs

```typescript
// services/global-manager/src/tasks/aggregate-logs.ts

export async function aggregateLogs() {
  // Get last processed timestamp
  const lastProcessed = await db.query.processing_state.findFirst({
    where: eq(processing_state.key, 'last_aggregated_log_timestamp')
  })

  const fromTimestamp = lastProcessed?.value || '2025-01-01T00:00:00Z'
  const toTimestamp = new Date().toISOString()

  // Aggregate into hourly buckets (idempotent via UPSERT)
  await db.execute(sql`
    INSERT INTO usage_hourly (hour, customer_id, service_type, request_count, bytes_in, bytes_out)
    SELECT
      date_trunc('hour', timestamp) as hour,
      customer_id,
      service_type,
      COUNT(*) as request_count,
      SUM(bytes_in) as bytes_in,
      SUM(bytes_out) as bytes_out
    FROM haproxy_logs
    WHERE timestamp > ${fromTimestamp}
      AND timestamp <= ${toTimestamp}
    GROUP BY hour, customer_id, service_type
    ON CONFLICT (hour, customer_id, service_type)
    DO UPDATE SET
      request_count = usage_hourly.request_count + EXCLUDED.request_count,
      bytes_in = usage_hourly.bytes_in + EXCLUDED.bytes_in,
      bytes_out = usage_hourly.bytes_out + EXCLUDED.bytes_out
  `)

  // Update last processed timestamp
  await db.insert(processing_state)
    .values({
      key: 'last_aggregated_log_timestamp',
      value: toTimestamp
    })
    .onConflictDoUpdate({
      target: processing_state.key,
      set: { value: toTimestamp, updated_at: new Date() }
    })
}
```

### Task: Calculate Bills

```typescript
// services/global-manager/src/tasks/calculate-bills.ts

export async function calculateBills() {
  // Get unbilled usage
  const unbilledUsage = await db.query.usage_hourly.findMany({
    where: eq(usage_hourly.billed, false),
    with: {
      customer: {
        columns: {
          id: true,
          subscription_tier: true,
          billing_rate: true
        }
      }
    }
  })

  // Group by customer and calculate charges
  const chargesByCustomer = groupAndCalculate(unbilledUsage)

  // Insert billing records (idempotent)
  for (const [customerId, charges] of chargesByCustomer) {
    await db.transaction(async (tx) => {
      // Insert billing record
      await tx.insert(billing_records)
        .values({
          customer_id: customerId,
          period_start: charges.periodStart,
          period_end: charges.periodEnd,
          total_requests: charges.totalRequests,
          total_bandwidth_gb: charges.totalBandwidthGB,
          amount_usd: charges.amountUSD,
          status: 'pending'
        })
        .onConflictDoNothing()

      // Mark usage as billed
      await tx.update(usage_hourly)
        .set({ billed: true, billed_at: new Date() })
        .where(
          and(
            eq(usage_hourly.customer_id, customerId),
            gte(usage_hourly.hour, charges.periodStart),
            lte(usage_hourly.hour, charges.periodEnd)
          )
        )
    })
  }
}
```

### Task: Generate MA_VAULT (Primary - Customer Configurations)

```typescript
// services/global-manager/src/tasks/generate-ma-vault.ts
import { execSync } from 'child_process'

export async function generateMAVault() {
  // Get all active customers with their services and API keys
  const customers = await db.query.customers.findMany({
    where: eq(customers.status, 'active'),
    with: {
      api_keys: true,
      service_instances: true
    }
  })

  // Build MA_VAULT data structure (key-value format for kvcrypt)
  const vaultData: Record<string, string> = {}

  for (const customer of customers) {
    // Get tier from service_instances (customers don't have tier directly)
    const sealService = customer.service_instances.find(s => s.service_type === 'seal')
    const tier = sealService?.tier || 'starter'

    // Store customer config as JSON string value
    // See SEAL_SERVICE_CONFIG.md for tier structure and rate limit definitions
    const customerConfig = {
      api_keys: customer.api_keys.map(k => k.api_key_fp),  // Use fingerprint, not hash
      tier,  // From service_instances.tier
      limits: getTierLimits(tier),  // Helper function returns limits per tier
      status: customer.status  // From customers.status (active/suspended/closed)
    }

    // Key format: "customer:{customer_id}"
    vaultData[`customer:${customer.customer_id}`] = JSON.stringify(customerConfig)
  }

  function getTierLimits(tier: string) {
    // See SEAL_SERVICE_CONFIG.md for tier-specific limits
    const tierConfig = {
      starter: { guaranteed_rps: 100, burst_rps: 0, burst_duration_sec: 0 },
      pro: { guaranteed_rps: 500, burst_rps: 200, burst_duration_sec: 60 },
      enterprise: { guaranteed_rps: 2000, burst_rps: 1000, burst_duration_sec: 300 }
    }
    return tierConfig[tier] || tierConfig.starter
  }

  // Generate content hash for change detection
  const vaultContent = JSON.stringify(vaultData, null, 2)
  const vaultHash = crypto.createHash('sha256').update(vaultContent).digest('hex')

  // Check if this version already exists
  const existing = await db.query.vault_versions.findFirst({
    where: eq(vault_versions.content_hash, vaultHash)
  })

  if (!existing) {
    // New version - write to MA_VAULT using kvcrypt
    for (const [key, value] of Object.entries(vaultData)) {
      try {
        // Use kvcrypt to store encrypted key-value pairs
        // This creates ma-{timestamp}-{hashes}.enc files in /opt/syncf/data_tx/ma/
        execSync(`/home/olet/walrus/scripts/sync/kvcrypt.py put ma "${key}" '${value}'`, {
          stdio: 'pipe'
        })
      } catch (error) {
        logger.error({ key, error }, 'Failed to write to MA_VAULT')
        throw error
      }
    }

    // Record version in database
    await db.insert(vault_versions).values({
      version: Date.now(),
      content_hash: vaultHash,
      customer_count: customers.length,
      created_at: new Date()
    })

    logger.info({ hash: vaultHash, count: customers.length }, 'New MA_VAULT version generated')

    // Note: sync-files.py (systemd timer) will automatically distribute
    // the ma-*.enc files to API Gateway servers via rsync
  } else {
    logger.info({ hash: vaultHash }, 'MA_VAULT unchanged, skipping generation')
  }
}
```

### Systemd Configuration

The Global Manager runs as a long-lived daemon process managed by systemd. No timer is needed since scheduling is handled internally via setInterval.

```ini
# /etc/systemd/system/suiftly-global-manager.service
[Unit]
Description=Suiftly Global Manager (Long-Lived Daemon)
After=network.target postgresql.service
Requires=postgresql.service

[Service]
Type=simple
User=deploy
Group=deploy
WorkingDirectory=/var/www/global-manager

Environment="NODE_ENV=production"
Environment="DATABASE_URL=postgresql://suiftly@localhost/suiftly_prod"

ExecStart=/usr/bin/node dist/index.js

# Restart policy (daemon should always be running)
Restart=always
RestartSec=10s

# Graceful shutdown
KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=35s

# Resource limits
MemoryMax=512M
CPUQuota=50%

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=suiftly-global-manager

# Security
PrivateTmp=yes
NoNewPrivileges=yes
ProtectSystem=strict
ProtectHome=yes
ReadWritePaths=/var/lib/suiftly/ma_vault /opt/syncf/data_tx

[Install]
WantedBy=multi-user.target
```

**Key Configuration Details:**
- `Type=simple` - Long-lived process (not oneshot)
- `Restart=always` - Automatically restart if process crashes
- `KillSignal=SIGTERM` - Send SIGTERM for graceful shutdown
- `TimeoutStopSec=35s` - Allow 35s for graceful shutdown (cycle may take up to 30s)
- No timer file needed - scheduling is internal via setInterval

---

## 2. MA_VAULT Distribution

The MA_VAULT generated by the Global Manager is distributed to API Gateway servers in the ~/walrus project using sync-files.py.

**Distribution Process:**
1. Global Manager writes MA_VAULT files to `/opt/syncf/data_tx/ma/` using kvcrypt
2. sync-files.py (separate systemd timer) syncs these files to API Gateway servers
3. Local Managers (in ~/walrus) read MA_VAULT and update HAProxy configurations

**Note:** Local Manager, HAProxy, and Fluentd configurations are part of the ~/walrus project and are not documented here.

---

## 3. Database Schema

The Global Manager uses the following database tables:

```sql
-- Processing state tracking
CREATE TABLE processing_state (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Worker run history
CREATE TABLE worker_runs (
  id SERIAL PRIMARY KEY,
  worker_type TEXT NOT NULL, -- 'global-manager', 'local-manager'
  server_name TEXT,
  task_name TEXT NOT NULL,
  status TEXT NOT NULL,
  duration_ms INT,
  error_message TEXT,
  executed_at TIMESTAMPTZ DEFAULT NOW()
);

-- HAProxy logs (TimescaleDB) - see CUSTOMER_SERVICE_SCHEMA.md for complete schema
CREATE TABLE haproxy_logs (
  timestamp TIMESTAMPTZ NOT NULL,
  customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
  api_key_id VARCHAR(100),
  service_type TEXT NOT NULL,
  endpoint TEXT NOT NULL,
  method TEXT NOT NULL,
  status_code INT,
  bytes_in BIGINT,
  bytes_out BIGINT,
  request_time_ms INT,
  backend_time_ms INT,
  haproxy_server TEXT,
  region TEXT
);

SELECT create_hypertable('haproxy_logs', 'timestamp', chunk_time_interval => INTERVAL '7 days');
SELECT add_compression_policy('haproxy_logs', INTERVAL '7 days');
SELECT add_retention_policy('haproxy_logs', INTERVAL '90 days');

-- Usage aggregates
CREATE TABLE usage_hourly (
  hour TIMESTAMPTZ NOT NULL,
  customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
  service_type TEXT NOT NULL,
  request_count BIGINT,
  bytes_in BIGINT,
  bytes_out BIGINT,
  error_count INT,
  billed BOOLEAN DEFAULT FALSE,
  billed_at TIMESTAMPTZ,
  PRIMARY KEY (hour, customer_id, service_type)
);

-- Billing records
CREATE TABLE billing_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
  period_start TIMESTAMPTZ NOT NULL,
  period_end TIMESTAMPTZ NOT NULL,
  total_requests BIGINT,
  total_bandwidth_gb NUMERIC(10,2),
  amount_usd NUMERIC(10,2),
  status TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(customer_id, period_start, period_end)
);

-- NOTE: For complete database schema, see CUSTOMER_SERVICE_SCHEMA.md
-- This section shows only the tables specific to Global Manager operations

-- Customer limits (see SEAL_SERVICE_CONFIG.md for tier definitions)
CREATE TABLE customer_limits (
  customer_id INTEGER PRIMARY KEY REFERENCES customers(customer_id),
  tier TEXT NOT NULL DEFAULT 'starter',  -- starter, pro, enterprise
  rate_limits JSONB DEFAULT '{}',      -- {guaranteed_rps, burst_rps, burst_duration_sec}
  quotas JSONB DEFAULT '{}',
  status TEXT DEFAULT 'active',
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- MA_VAULT versions
CREATE TABLE vault_versions (
  id SERIAL PRIMARY KEY,
  version BIGINT NOT NULL,
  content_hash TEXT NOT NULL UNIQUE,
  encrypted_content BYTEA,
  customer_count INT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## Deployment

### Install Global Manager (on DB server)

```bash
# Build
cd services/global-manager
npm run build

# Deploy
scp -r dist/ db1.suiftly.io:/var/www/global-manager/
scp systemd/* db1.suiftly.io:/tmp/

# Install systemd service
ssh db1.suiftly.io '
  sudo cp /tmp/suiftly-global-manager.service /etc/systemd/system/
  sudo systemctl daemon-reload
  sudo systemctl enable --now suiftly-global-manager.service
'
```

**Note:** Local Manager deployment is handled in the ~/walrus project.

---

## Monitoring & Health Checks

### Admin Dashboard

Simple HTML dashboard for debugging and monitoring. Designed for both human operators and AI agents (via WebFetch).

**Architecture:** Server-side rendered HTML (multi-page)
- Templates are TypeScript functions returning HTML strings
- Zero build step for instant iteration
- Evolves as debugging needs change

**Server:**
- Port: 3001 (localhost only)
- Route: `GET /` for main dashboard
- Additional routes added as needed (`/logs`, `/tasks/:id`, etc.)

**Project Structure:**

```
services/global-manager/
├─ src/
│  ├─ index.ts              # Main daemon (scheduler loop)
│  ├─ admin-server.ts       # Admin HTTP server (port 3001)
│  ├─ admin/                # HTML templates (TypeScript functions)
│  │  ├─ layout.ts          # Shared HTML layout
│  │  ├─ dashboard.ts       # Main dashboard view
│  │  └─ (add views as needed)
│  ├─ tasks/                # Worker tasks
│  └─ lib/                  # Utilities
```

**Implementation:**

```typescript
// services/global-manager/src/admin-server.ts

import Fastify from 'fastify'
import { db, eq, desc } from '@suiftly/database'
import { execSync } from 'child_process'
import { dashboardView } from './admin/dashboard'

const fastify = Fastify({ logger: true })

// Main dashboard
fastify.get('/', async (req, reply) => {
  // Fetch health data
    const lastRuns = await db.query.worker_runs.findMany({
      where: eq(worker_runs.worker_type, 'global-manager'),
      orderBy: desc(worker_runs.executed_at),
      limit: 20
    })

    const isRunning = await checkServiceStatus()
    const lastSuccess = lastRuns.find(r => r.status === 'success')
    const recentFailures = lastRuns.filter(r => r.status === 'failed')
    const slowTasks = lastRuns.filter(r => r.duration_ms > 10000)

    // Return simple HTML (no fancy UI, evolves as needed)
    return reply.type('text/html').send(`
<!DOCTYPE html>
<html>
<head>
  <title>Global Manager Health</title>
  <style>
    body { font-family: monospace; margin: 2rem; }
    .ok { color: green; }
    .warn { color: orange; }
    .error { color: red; }
    table { border-collapse: collapse; margin-top: 1rem; }
    th, td { padding: 0.5rem; text-align: left; border: 1px solid #ccc; }
  </style>
</head>
<body>
  <h1>Global Manager Health</h1>

  <h2 class="${isRunning ? 'ok' : 'error'}">
    ${isRunning ? '✓ Running' : '✗ Stopped'}
  </h2>

  ${lastSuccess ? `
    <p>Last success: ${formatTime(lastSuccess.executed_at)} (${lastSuccess.duration_ms}ms)</p>
  ` : '<p class="error">No successful runs</p>'}

  ${recentFailures.length > 0 ? `
    <h3 class="error">Recent Failures (${recentFailures.length})</h3>
    <ul>
      ${recentFailures.map(f => `<li>${f.task_name}: ${f.error_message}</li>`).join('')}
    </ul>
  ` : ''}

  ${slowTasks.length > 0 ? `
    <h3 class="warn">Slow Tasks (> 10s)</h3>
    <ul>
      ${slowTasks.map(t => `<li>${t.task_name}: ${t.duration_ms}ms</li>`).join('')}
    </ul>
  ` : ''}

  <h3>Recent Runs</h3>
  <table>
    <tr><th>Time</th><th>Task</th><th>Status</th><th>Duration</th><th>Error</th></tr>
    ${lastRuns.map(r => `
      <tr>
        <td>${formatTime(r.executed_at)}</td>
        <td>${r.task_name}</td>
        <td class="${r.status === 'success' ? 'ok' : 'error'}">${r.status}</td>
        <td>${r.duration_ms || '-'}ms</td>
        <td>${r.error_message || '-'}</td>
      </tr>
    `).join('')}
  </table>

  <script>setTimeout(() => location.reload(), 30000)</script>
</body>
</html>
    `)
})

function checkServiceStatus(): boolean {
  try {
    execSync('systemctl is-active suiftly-global-manager.service')
    return true
  } catch {
    return false
  }
}

function formatTime(date: Date): string {
  const ago = Math.floor((Date.now() - date.getTime()) / 1000)
  if (ago < 60) return `${ago}s ago`
  if (ago < 3600) return `${Math.floor(ago / 60)}m ago`
  return `${Math.floor(ago / 3600)}h ago`
}

// Start admin server on dedicated port
fastify.listen({ port: 3001, host: '127.0.0.1' }, (err) => {
  if (err) {
    fastify.log.error(err)
    process.exit(1)
  }
  fastify.log.info('Admin dashboard listening on http://localhost:3001')
})
```

**Example Template (TypeScript function returning HTML):**

```typescript
// services/global-manager/src/admin/dashboard.ts

export function dashboardView(data: {
  isRunning: boolean
  lastSuccess?: any
  recentFailures: any[]
  lastRuns: any[]
}): string {
  return `
<!DOCTYPE html>
<html>
<head>
  <title>Global Manager - Dashboard</title>
  <style>
    body { font-family: monospace; margin: 2rem; background: #1e1e1e; color: #d4d4d4; }
    .ok { color: #4caf50; }
    .error { color: #f44336; }
    table { border-collapse: collapse; width: 100%; margin-top: 1rem; }
    th, td { padding: 0.5rem; text-align: left; border: 1px solid #3e3e3e; }
  </style>
</head>
<body>
  <h1>Global Manager Health</h1>
  <h2 class="${data.isRunning ? 'ok' : 'error'}">
    ${data.isRunning ? '✓ Running' : '✗ Stopped'}
  </h2>
  ${data.lastSuccess ? `
    <p>Last success: ${formatTime(data.lastSuccess.executed_at)}
       (${data.lastSuccess.duration_ms}ms)</p>
  ` : '<p class="error">No successful runs</p>'}

  <!-- Add more sections as debugging needs evolve -->

  <script>setTimeout(() => location.reload(), 30000)</script>
</body>
</html>
  `
}
```

**Features:**
- Service status (running/stopped via systemd)
- Last successful run time and duration
- Recent failures with error messages
- Slow tasks (> 10s threshold)
- Last 20 runs with details
- Auto-refresh every 30 seconds

**Usage:**
- Humans: Browse to http://localhost:3001/
- AI agents: Use WebFetch tool to read and interpret status
- No authentication needed (dedicated admin port, internal only)

**Design Philosophy:**
- Minimal HTML/CSS (evolves based on debugging needs)
- Optimized for readability (humans + AI agents)
- No API needed (HTML provides all context)

### Health Check Endpoints

Lightweight JSON endpoints for external monitoring tools:

```typescript
// apps/api/src/routes/health.ts

// Quick health check (for uptime monitoring)
router.get('/health/global-manager', async (req, res) => {
  const lastRun = await db.query.worker_runs.findFirst({
    where: and(
      eq(worker_runs.worker_type, 'global-manager'),
      eq(worker_runs.status, 'success')
    ),
    orderBy: desc(worker_runs.executed_at)
  })

  const minutesSinceRun = (Date.now() - lastRun.executed_at.getTime()) / 60000

  if (minutesSinceRun > 15) {
    return res.status(503).send({
      status: 'unhealthy',
      message: `Global manager hasn't run in ${minutesSinceRun} minutes`
    })
  }

  return res.send({ status: 'healthy', last_run: lastRun.executed_at })
})
```

### Monitoring Commands

```bash
# Check global manager daemon status
systemctl status suiftly-global-manager.service
journalctl -u suiftly-global-manager -f

# Restart daemon
systemctl restart suiftly-global-manager.service

# Stop daemon (for maintenance)
systemctl stop suiftly-global-manager.service

# Check worker run history
psql suiftly_prod -c "SELECT * FROM worker_runs WHERE worker_type='global-manager' ORDER BY executed_at DESC LIMIT 10;"
```

---

## Testing Strategy

### Unit Tests

```typescript
// services/global-manager/tests/tasks.test.ts
test('aggregateLogs is idempotent', async () => {
  await aggregateLogs()
  await aggregateLogs()

  const usage = await db.query.usage_hourly.findMany()
  expect(new Set(usage.map(u => u.id)).size).toBe(usage.length)
})

test('vault generation produces deterministic output', async () => {
  const vault1 = await generateVault()
  const vault2 = await generateVault()

  expect(vault1.hash).toBe(vault2.hash)
})
```

### Integration Tests

```typescript
// tests/integration/workers.test.ts
test('end-to-end metering flow', async () => {
  // 1. Insert test logs
  await insertTestLogs()

  // 2. Run global manager
  await runGlobalManager()

  // 3. Verify aggregates created
  const aggregates = await db.query.usage_hourly.findMany()
  expect(aggregates).toHaveLength(10)

  // 4. Verify billing records
  const bills = await db.query.billing_records.findMany()
  expect(bills).toHaveLength(3)

  // 5. Verify vault generated
  const vault = await fs.readFile('/var/lib/suiftly/ma_vault/latest.enc')
  expect(vault).toBeDefined()
})
```

---

## Security Considerations

1. **MA_VAULT Encryption** - AES-256-GCM with rotating keys
2. **PostgreSQL Advisory Locks** - Prevent concurrent execution
3. **Systemd Security** - ProtectSystem, PrivateTmp, limited permissions
4. **API Key Hashing** - Only store hashed keys in logs
5. **Rate Limit Bypass** - Internal services use separate backend
6. **Audit Trail** - All configuration changes logged

---

## Performance Optimizations

1. **Continuous Aggregates** - Pre-computed usage metrics
2. **Batch Processing** - Process logs in chunks
3. **Connection Pooling** - Reuse database connections
4. **Incremental Updates** - Only process new data
5. **Content Hashing** - Skip unchanged vault updates

---

## Disaster Recovery

### Backup Strategy

```bash
# Backup worker state and configs
pg_dump -t processing_state -t worker_runs -t vault_versions > worker_state.sql

# Restore on new server
psql suiftly_prod < worker_state.sql

# Restart daemon
systemctl restart suiftly-global-manager.service
```

### Failure Recovery

- **Global Manager Failure** - Advisory lock expires, next run continues
- **Local Manager Failure** - HAProxy continues with last known config
- **Database Failure** - Workers retry with exponential backoff
- **Network Partition** - Local managers use cached vault

---

## Future Enhancements

1. **Real-time Updates** - WebSocket push for instant rate limit changes
2. **Multi-region** - Regional aggregation before global billing
3. **Machine Learning** - Anomaly detection for usage patterns
4. **GraphQL Subscriptions** - Live usage updates in dashboard
5. **Kubernetes Operators** - Cloud-native deployment option

---

## Appendix: MM_VAULT (Encryption Keys Management)

### Overview

MM_VAULT (Mainnet-Master vault) contains encryption keys that customers have imported for use with the Seal service. This vault is:
- **Significantly smaller** than MA_VAULT (only customers with imported keys)
- **Less frequently updated** (only when customers import/update keys)
- **Read by key-servers on restart** (not by HAProxy/Local Managers)
- **Not critical for initial MVP** (key import feature can be added later)

### MM_VAULT Generation

The Global Manager also generates MM_VAULT alongside MA_VAULT:

```typescript
// services/global-manager/src/tasks/generate-mm-vault.ts

export async function generateMMVault() {
  // Get customers with imported encryption keys
  const importedKeys = await db.query.customer_encryption_keys.findMany({
    where: eq(customer_encryption_keys.status, 'active'),
    with: {
      customer: true
    }
  })

  // Build MM_VAULT data structure
  const mmVaultData: Record<string, string> = {}

  for (const keyRecord of importedKeys) {
    // Store encryption key config
    const keyConfig = {
      customer_id: keyRecord.customer_id,
      key_id: keyRecord.key_id,
      key_type: keyRecord.key_type,
      encrypted_key: keyRecord.encrypted_key_material,
      metadata: keyRecord.metadata,
      created_at: keyRecord.created_at
    }

    // Key format: "key:{customer_id}:{key_id}"
    mmVaultData[`key:${keyRecord.customer_id}:${keyRecord.key_id}`] = JSON.stringify(keyConfig)
  }

  // Only write if there are keys to store
  if (Object.keys(mmVaultData).length > 0) {
    // Write to MM_VAULT using kvcrypt
    for (const [key, value] of Object.entries(mmVaultData)) {
      execSync(`/home/olet/walrus/scripts/sync/kvcrypt.py put mm "${key}" '${value}'`, {
        stdio: 'pipe'
      })
    }

    logger.info({ keyCount: Object.keys(mmVaultData).length }, 'MM_VAULT updated with imported keys')
  }
}
```

### MM_VAULT Distribution and Usage

1. **Distribution**: Same as MA_VAULT - sync-files.py distributes to all servers
2. **Consumption**: Key-servers read MM_VAULT on startup to load customer encryption keys
3. **Security**: Even more restricted access than MA_VAULT (mm-readers group)

### Database Schema for Key Management

```sql
-- Customer encryption keys (for MM_VAULT)
CREATE TABLE customer_encryption_keys (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  customer_id INTEGER NOT NULL REFERENCES customers(customer_id),  -- FIX: Use INTEGER, not UUID
  key_id TEXT NOT NULL,
  key_type TEXT NOT NULL, -- 'aes256', 'rsa4096', etc
  encrypted_key_material TEXT NOT NULL, -- The actual encrypted key
  metadata JSONB DEFAULT '{}',
  status TEXT DEFAULT 'active',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(customer_id, key_id)
);
```

### Timeline

- **Phase 1 (MVP)**: Focus on MA_VAULT only, no key import support
- **Phase 2**: Add key import UI and MM_VAULT generation
- **Phase 3**: Full key lifecycle management (rotation, revocation)